{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe5dfbc-0ff6-4c2d-8efb-5b28aaf7060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aed5ae2-7ebb-490f-a365-253d8dabf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [x * 0.1 for x in range(-10, 11, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99d24cf1-4885-4cd4-ac12-1d05b0b4a23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " -0.9,\n",
       " -0.8,\n",
       " -0.7000000000000001,\n",
       " -0.6000000000000001,\n",
       " -0.5,\n",
       " -0.4,\n",
       " -0.30000000000000004,\n",
       " -0.2,\n",
       " -0.1,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6000000000000001,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 1.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b3f4f3-8f35-4ef9-ad54-62670f3be3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    ALPHA = 0.1\n",
    "    EPOCHS = 1000\n",
    "    BATCH_SIZE = 4096\n",
    "    INPUT_DIM = 5 # portfolio_amount, stock_price, negative_score, neutral_score, positive_score\n",
    "    OUTPUT_DIM = len(actions) # see actions above\n",
    "    MEMORY_SIZE = 20000 # Number of state transitions to store\n",
    "    NUM_ACTIONS = OUTPUT_DIM\n",
    "    GAMMA = 0.99 # Discount factor\n",
    "    \n",
    "    memory = [None] * MEMORY_SIZE # List of state transitions, not stored in order of trajectory\n",
    "    model = None # Q learning model\n",
    "    target_model = None # Q learning model\n",
    "    modelPath = None\n",
    "    fig_count = 0\n",
    "    \n",
    "    def __init__(self, modelPath = None):\n",
    "        self.modelPath = modelPath\n",
    "        if(self.modelPath != None):\n",
    "            try:\n",
    "                self.model = load_model(modelPath)\n",
    "                print(\"Model Successfully Loaded\")\n",
    "            except:\n",
    "                self.model = None\n",
    "        else:\n",
    "            self.modelPath = \"model.h5\"\n",
    "        if(self.model == None):\n",
    "            self.model = self.createModel()\n",
    "        self.target_model = self.createModel()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def createModel(self):\n",
    "        init = tf.keras.initializers.HeUniform()\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim = self.INPUT_DIM, activation='relu', kernel_initializer=init))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer=init))\n",
    "        model.add(Dense(self.OUTPUT_DIM, kernel_initializer=init))\n",
    "        \n",
    "        model.compile(loss = tf.keras.losses.MeanSquaredError(), optimizer='adam')\n",
    "        return model\n",
    "    \n",
    "    def replay(self):\n",
    "        memory = []\n",
    "        for m in self.memory:\n",
    "            if(m != None):\n",
    "                memory.append(m)\n",
    "        random.shuffle(memory)\n",
    "        # each element in memory is state0, action, state1, reward, terminal\n",
    "        s0 = [m[0] for m in memory]\n",
    "        s1 = [m[2] for m in memory]\n",
    "        q0 = self.model.predict(np.array(s0))\n",
    "        q1 = self.target_model.predict(np.array(s1))\n",
    "        \n",
    "        # Apply bellman ford\n",
    "        for i in range(len(memory)):\n",
    "            action = memory[i][1]\n",
    "            reward = memory[i][3]\n",
    "            terminal = memory[i][4]\n",
    "            if(terminal):\n",
    "                q0[i][action] += self.ALPHA * (reward - q0[i][action])\n",
    "            else:\n",
    "                q0[i][action] += self.ALPHA * (reward + self.GAMMA * max(q1[i]) - q0[i][action])\n",
    "        \n",
    "        history = self.model.fit(np.array(s0), q0, batch_size = self.BATCH_SIZE, epochs=self.EPOCHS, verbose=0)\n",
    "        history = history.history['loss']\n",
    "        sns.lineplot(x = range(len(history)), y = history)\n",
    "        plt.savefig(f\"figures/fig_{self.fig_count}.png\")\n",
    "        plt.clf()\n",
    "        self.fig_count += 1\n",
    "        \n",
    "    \n",
    "    def updateModel(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # save model\n",
    "        self.target_model.save(self.modelPath)\n",
    "    \n",
    "    def parseState(self, gameState):\n",
    "        return gameState\n",
    "    \n",
    "    def addStateTransition(self, stateTransition):\n",
    "        for i in range(len(self.memory)):\n",
    "            if self.memory[i] == None:\n",
    "                self.memory[i] = stateTransition\n",
    "                return\n",
    "        del self.memory[random.randint(0, len(self.memory) - 1)] #remove one element\n",
    "        self.memory.append(stateTransition)\n",
    "    \n",
    "    def stateTransition(self, gs0, action, gs1, reward, terminal):\n",
    "        st = [self.parseState(gs0), action, self.parseState(gs1), reward, terminal]\n",
    "        self.addStateTransition(st)\n",
    "    \n",
    "    def getAction(self, gs, ep = 0):\n",
    "        if random.random() < ep:\n",
    "            return random.randrange(0, self.NUM_ACTIONS)\n",
    "        s = self.parseState(gs)\n",
    "        q = self.model.predict([s])\n",
    "        \n",
    "        return np.argmax(q[0])\n",
    "    \n",
    "    def parseAction(self, action):\n",
    "        a = actions[action]\n",
    "        return [0 if a < 0 else (1 if a == 0 else 2), abs(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ee6b9-95a4-4267-b4e6-55ab71c5c7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
